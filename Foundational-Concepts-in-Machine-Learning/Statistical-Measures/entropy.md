Entropy: A theoretical measure of uncertainty within a single distribution, not typically used as a loss function.

Cross-Entropy: A measure of the difference between two distributions, widely used as a loss function in machine learning for classification tasks due to its properties of penalizing incorrect predictions and being suitable for probability distributions.
