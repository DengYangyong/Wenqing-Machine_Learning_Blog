<img src="mean_std.png" alt="mean_std" width="400" height="300"/>
> Note: The output layer's neuron count often differs from the input layer. Each layer can have a distinct number of neurons, indicating its 'size' or capacity.
>
## Reference:
- [Watch the video on YouTube](https://youtube.com/watch?v=jTzJ9zjC8nU)
$\text{SSR}(a, b) = \sum\limits_{i=1}^{n} (y_i - ax_i - b)^2$


ffnn.md✅
linear-regression.md✅
variance&standarDeviation.md✅
Covariance.md ❌ need further improvement for pca
Linearity.md❌ need further improvement for why the product of the standard deviations of x and y is normalization? Which is correlated to correlation.
p_values.md✅
Correlation.md✅
r-squared.md✅
statistical-significance.md✅
Standard-Deviation-vs-Standard-Error.md✅
Confidence-Intervals.md✅
least-squared.md✅
calculus.md✅
gradient-descent.md✅ need further math formula format improvement, need format and refrase
Neural-Network.md✅ need format and rephrase
backpropagation.md✅ need format and rephrase
relu.md✅ need format and rephrase
activation-functions.md✅ need format and rephrase


Activation function
Chain rule
logistic regression
t-sne
implicit function
scalar function
Directional Derivative:


why gradient" refers to a vector that points in the direction of the greatest rate of increase  give specific example???
why the product of the standard deviations of x and y is normalization?
